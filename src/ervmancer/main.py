import argparse
import logging
import os
import sys
import datetime
import subprocess
import pkg_resources
from tqdm import tqdm
from .preprocessing.filter_reads import ReadFilter
from .preprocessing.subset_reads import extract_out_original_reads_by_subset
from .kmer.process_kmer import extract_out_original_reads_by_subset
import hashlib
import random
import string


def generate_random_hash(length: int = 8):
    """Generate a random hash string for file naming."""
    # Create a random string
    random_str = ''.join(random.choices(
        string.ascii_lowercase + string.digits, k=16))
    # Create a hash from it
    hash_obj = hashlib.md5(random_str.encode())
    # Return the first few characters of the hash
    return hash_obj.hexdigest()[:length]


def get_unique_id():
    """Generate a unique ID for file naming."""
    return generate_random_hash()


def run_command(cmd: str):
    try:
        subprocess.run(cmd, shell=True, check=True)
    except subprocess.CalledProcessError as e:
        logging.error(f"Command failed: {cmd}")
        raise e


def get_data_path(filename: str):
    """Get the path to a data file included in the package."""
    try:
        # Installed package - ERVmancer
        return pkg_resources.resource_filename('ervmancer', os.path.join('data', filename))
    except (ImportError, ModuleNotFoundError):
        # Developer Mode
        return os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data', filename)


def cleanup_intermediate_files(output_pathname: str, keep_intermediate: bool):
    # Cleanup files.
    cleanup_dir = os.path.join(output_pathname, 'intermediate_files')
    if not keep_intermediate:
        logging.info("Cleaning up intermediate files...")
        if os.path.exists(cleanup_dir):
            for filename in os.listdir(cleanup_dir):
                file_path = os.path.join(cleanup_dir, filename)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        logging.debug(
                            f"Removed intermediate file: {file_path}")
                except Exception as e:
                    logging.error(f"Error removing {file_path}: {e}")
            logging.info(f"Cleanup of {cleanup_dir} complete")
        else:
            logging.warning(
                f"Cleanup directory {cleanup_dir} does not exist or is not a directory")
    logging.info("Processing complete!")


def main():
    parser = argparse.ArgumentParser(description='ervmancer')
    parser.add_argument('--b', type=str, required=False,
                        help="User provided absolute path to self provided bowtie2 alignment file")
    parser.add_argument('--r1', required=False,
                        help='Absolute path to paired-end R1 fastq file.')
    parser.add_argument('--r2', required=False,
                        help='Absolute path to paired-end R2 fastq file.')
    parser.add_argument('--s1', required=False,
                        help='Absolute path to single strand S1 fastq file.')
    parser.add_argument('--keep_files', action='store_true', default=False,
                        help="Keeps intermediate outputs from Samtools and Bedtools.")
    parser.add_argument('--num_cores', type=int, default=8,
                        help="Number of CPU cores used for processing.")
    parser.add_argument('--output_dir', required=True,
                        help='Absolute path to base output directory for CSV output and intermediate files. (Output folders are autogenerated if they do not exist already!)')
    parser.add_argument('--bowtie_index', required=True,
                        help="Absolute path to the Bowtie2 index to be used for processing.")

    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(levelname)s - %(message)s', force=True)

    try:
        # Create output directories
        for subdir in ['intermediate_files', 'final', 'logs']:
            os.makedirs(os.path.join(args.output_dir, subdir), exist_ok=True)

        read_filter = ReadFilter(args.output_dir, args.r1, args.r2, args.s1)
        base_name, paired = read_filter.validate_inputs()
        logging.info(f'Base Name: {base_name}')

        os.makedirs(os.path.join(args.output_dir,
                    "intermediate_files"), exist_ok=True)
        unique_id = get_unique_id()

        if args.b:
            try:
                # Convert to absolute path
                outsam_pathname = os.path.abspath(args.b)
                # Check if the path exists
                if os.path.exists(outsam_pathname):
                    # Parse given path as an absolute path for consumption
                    logging.info(
                        f"Using user provided alignment file: {outsam_pathname}")
                else:
                    logging.info(
                        "Path to provided alignment file is invalid. File does not exist. Terminating program.")
                    sys.exit(1)
            except Exception as e:
                logging.error(f"Error processing path: {str(e)}")
                sys.exit(1)
        else:
            outsam_pathname = read_filter.get_path(
                'intermediate_files', f'{base_name}_{unique_id}_bowtie2_out', 'sam')

        outbam_pathname = read_filter.get_path(
            'intermediate_files', f'{base_name}_{unique_id}_all_hervs', 'bam')
        sorted_outbam_pathname = read_filter.get_path(
            'intermediate_files', f'{base_name}_{unique_id}_sorted', 'bam')
        converted_herv_gtf_to_bed = get_data_path('hervs_genomic_coords.bed')
        logging.info(f"Using HERV BED file: {converted_herv_gtf_to_bed}")
        outbed_pathname = read_filter.get_path(
            'final', f'{base_name}_{unique_id}_multimap', 'bed')
        # Bedtools intersect with HERV GTF file (Multimap)
        subset_outsam_pathname = read_filter.get_path(
            'final', f'{base_name}_{unique_id}_hervs_subset', 'sam')
        # Filter unique read appearances (step for KMER)
        subset_outbam_pathname = read_filter.get_path(
            'intermediate_files', f'{base_name}_{unique_id}_all_hervs.bwa_read', 'bam')

        commands = [
            f"samtools view -bS {outsam_pathname} -o {outbam_pathname}",
            f"samtools sort -@ {args.num_cores} {outbam_pathname} -o {sorted_outbam_pathname}",
            f"samtools index {sorted_outbam_pathname}",
            # Multimap uses this intermediate output below in the final dir
            f"bedtools intersect -abam {sorted_outbam_pathname} -b {converted_herv_gtf_to_bed} -wa -wb -bed > {outbed_pathname}",
            f"bedtools intersect -abam {sorted_outbam_pathname} -b {converted_herv_gtf_to_bed} > {subset_outbam_pathname}",
            # Kmer step uses this intermediate output in the final dir
            f"samtools view -h -o {subset_outsam_pathname} {subset_outbam_pathname}"
        ]
        if not args.b:
            # if bowtie file is not provided by user, add the bowtie 2 commands with unified mode parameters
            if paired:
                logging.info("Bowtie 2 - Unified Mode, paired reads")
                bt_cmd = f"""bowtie2 -p {args.num_cores} --very-sensitive --end-to-end -X 1500 \
                --no-mixed --no-discordant --no-dovetail --no-unal --score-min L,-0.1,-0.1 \
                -x {args.bowtie_index} -1 {read_filter.r1_path} -2 {read_filter.r2_path} -k 100 -S {outsam_pathname} \
                2> {read_filter.get_path('logs', base_name, f'bt2_paired_{unique_id}.err')}"""
            else:
                logging.info("Bowtie 2 - Unified Mode, single strand")
                bt_cmd = f"""bowtie2 -p {args.num_cores} -N 1 -L 10 --very-sensitive --end-to-end --no-unal \
                -x {args.bowtie_index} --score-min L,-0.1,-0.1 -U {read_filter.s1_path} -k 100 -S {outsam_pathname} \
                2> {read_filter.get_path('logs', base_name, f'bt2_single_{unique_id}.err')}"""
            # insert the bowtie 2 command to the front of the pipeline commands queue
            commands.insert(0, bt_cmd)

        logging.info("Starting processing pipeline...")
        for cmd in tqdm(commands, desc="Processing commands"):
            logging.info(f"Executing: {cmd}")
            run_command(cmd)

        pathname_extracted_reads = read_filter.get_path(
            'intermediate_files', f'{base_name}_{unique_id}_extracted_reads', 'txt')
        # filter each read so that it appears once for KMER method
        logging.info("Extracting original reads for kmer analysis...")
        if paired:
            extract_out_original_reads_by_subset(
                read_filter.r1_path,
                subset_outsam_pathname,
                pathname_extracted_reads,
                read_filter.r2_path
            )
        else:
            extract_out_original_reads_by_subset(
                read_filter.s1_path,
                subset_outsam_pathname,
                pathname_extracted_reads
            )

        # KMER STEP: Returns a dictionary of reads as keys and kmer assignment as values
        read_to_kmer_assignment_dict = parse_fastq_run_kmer_return_dict(input_sam_pathname=pathname_extracted_reads,
                                                                        kmer_herv_dict=kmer_herv_dict,
                                                                        path_dict=herv_path_dict,
                                                                        under_clade_dict=clades_under_dict,
                                                                        kmer_size=31,
                                                                        paired_end=paired_end)
        cleanup_intermediate_files(args.output_dir, args.keep_files)

    except Exception as e:
        logging.error(f"An error occurred: {str(e)}")
        raise


if __name__ == "__main__":
    main()
