import argparse
import logging
import os
import sys
import pickle
import subprocess
import pkg_resources
from tqdm import tqdm
from ervmancer.preprocessing.filter_reads import ReadFilter
from ervmancer.preprocessing.subset_reads import extract_out_original_reads_by_subset
from ervmancer.kmer.process_kmer import parse_fastq_run_kmer_return_dict
from ervmancer.multimap.multimapped_sam import single_multimapped_sam_to_dictionary
from ervmancer.resolve_reads.resolve_and_export import resolve_reads_single_sample_final, convert_final_dict_into_csv
from ervmancer.resolve_reads.other_methods import assign_tree_for_other_methods
import hashlib
import random
import string


def retrieve_pickled_python_obj(pathname):
    '''
    Gets a pickled python object at the path and name
    :param pathname: the path and name to the matrix, make sure it has the .pickle at the end.
    :return: whatever object is at the path and name location
    '''
    with open(pathname, "rb") as handle:
        pickled_obj = pickle.load(handle)
    return pickled_obj


def generate_random_hash(length: int = 8):
    """Generate a random hash string for file naming."""
    # Create a random string
    random_str = ''.join(random.choices(
        string.ascii_lowercase + string.digits, k=16))
    # Create a hash from it
    hash_obj = hashlib.md5(random_str.encode())
    # Return the first few characters of the hash
    return hash_obj.hexdigest()[:length]


def get_unique_id():
    """Generate a unique ID for file naming."""
    return generate_random_hash()


def run_command(cmd: str):
    try:
        subprocess.run(cmd, shell=True, check=True)
    except subprocess.CalledProcessError as e:
        logging.error(f"Command failed: {cmd}")
        raise e


def get_data_path(filename: str):
    """Get the path to a data file included in the package."""
    try:
        # Installed package - ERVmancer
        return pkg_resources.resource_filename('ervmancer', os.path.join('data', filename))
    except (ImportError, ModuleNotFoundError):
        # Developer Mode
        return os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data', filename)


def cleanup_intermediate_files(output_pathname: str, keep_intermediate: bool):
    # Cleanup files.
    cleanup_dir = os.path.join(output_pathname, 'intermediate_files')
    if not keep_intermediate:
        logging.info("Cleaning up intermediate files...")
        if os.path.exists(cleanup_dir):
            for filename in os.listdir(cleanup_dir):
                file_path = os.path.join(cleanup_dir, filename)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        logging.debug(
                            f"Removed intermediate file: {file_path}")
                except Exception as e:
                    logging.error(f"Error removing {file_path}: {e}")
            logging.info(f"Cleanup of {cleanup_dir} complete")
        else:
            logging.warning(
                f"Cleanup directory {cleanup_dir} does not exist or is not a directory")
    logging.info("Processing complete!")


def main():
    parser = argparse.ArgumentParser(description='ervmancer')
    parser.add_argument('--b', type=str, required=False,
                        help="User provided absolute path to self provided bowtie2 alignment file.")
    parser.add_argument('--advanced', type=str, required=False,
                        help="User provided absolute path to CSV file with user-provided read counts from other methods.")
    parser.add_argument('--r1', required=False,
                        help='Absolute path to paired-end R1 fastq file.')
    parser.add_argument('--r2', required=False,
                        help='Absolute path to paired-end R2 fastq file.')
    parser.add_argument('--s1', required=False,
                        help='Absolute path to single strand S1 fastq file.')
    parser.add_argument('--keep_files', action='store_true', default=False,
                        help="Keeps intermediate outputs from Samtools and Bedtools.")
    parser.add_argument('--num_cores', type=int, default=8,
                        help="Number of CPU cores used for processing.")
    parser.add_argument('--output_dir', required=True,
                        help='Absolute path to base output directory for CSV output and intermediate files. (Output folders are autogenerated if they do not exist already!)')
    parser.add_argument('--bowtie_index', required=True,
                        help="Absolute path to the Bowtie2 index to be used for processing.")

    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(levelname)s - %(message)s', force=True)

    # PROVIDED DICTIONARIES
    kmer_herv_dict_pathname = get_data_path(
        'clean_kmer_31_60_percent_cutoff.pickle')
    clades_under_dict_pathname = get_data_path(
        'cleaned_clades_under_dict.pkl')
    herv_path_dict_pathname = get_data_path('herv_path_dict.pkl')

    kmer_herv_dict = retrieve_pickled_python_obj(kmer_herv_dict_pathname)
    clades_under_dict = retrieve_pickled_python_obj(clades_under_dict_pathname)
    herv_path_dict = retrieve_pickled_python_obj(herv_path_dict_pathname)

    final_csv_out = read_filter.get_path(
        'final', f'{base_name}_{unique_id}_unified_run_final_out', 'csv')

    # ENTRYPOINT THREE - USER PROVIDED READS (FROM OTHER METHODS)
    if os.path.isabs(args.advanced) and os.path.exists(args.advanced) and os.path.isfile(args.advanced):
        # if the given absolute path is valid/exists
        logging.info(f"Assigning tree for given CSV file: {args.advanced}")
        assign_tree_for_other_methods(
            args.advanced, final_csv_out, clades_under_dict, herv_path_dict)
        logging.info(
            f"Assigning reads to tree for {args.advanced} and exporting output complete! Terminating program.")
        sys.exit()
    else:
        logging.error(
            "Invalid Path - please provide absolute path to CSV file.")

    if not os.path.exists(args.bowtie_index):
        logging.error("Bowtie index path does not exist or is invalid.")
        sys.exit()

    try:
        # Create output directories if they do not exist
        for subdir in ['intermediate_files', 'final', 'logs']:
            os.makedirs(os.path.join(args.output_dir, subdir), exist_ok=True)

        read_filter = ReadFilter(args.output_dir, args.r1, args.r2, args.s1)
        base_name, paired = read_filter.validate_inputs()
        logging.info(f'Base Name: {base_name}')

        os.makedirs(os.path.join(args.output_dir,
                    "intermediate_files"), exist_ok=True)
        unique_id = get_unique_id()

        if args.b:
            try:
                # Convert to absolute path
                outsam_pathname = os.path.abspath(args.b)
                # Check if the path exists
                if os.path.exists(outsam_pathname):
                    # Parse given path as an absolute path for consumption
                    logging.info(
                        f"Using user provided alignment file: {outsam_pathname}")
                else:
                    logging.info(
                        "Path to provided alignment file is invalid. File does not exist. Terminating program.")
                    sys.exit(1)
            except Exception as e:
                logging.error(f"Error processing path: {str(e)}")
                sys.exit(1)
        else:
            outsam_pathname = read_filter.get_path(
                'intermediate_files', f'{base_name}_{unique_id}_bowtie2_out', 'sam')

        outbam_pathname = read_filter.get_path(
            'intermediate_files', f'{base_name}_{unique_id}_all_hervs', 'bam')
        sorted_outbam_pathname = read_filter.get_path(
            'intermediate_files', f'{base_name}_{unique_id}_sorted', 'bam')
        converted_herv_gtf_to_bed = get_data_path('hervs_genomic_coords.bed')
        logging.info(f"Using HERV BED file: {converted_herv_gtf_to_bed}")
        outbed_pathname = read_filter.get_path(
            'intermediate_files', f'{base_name}_{unique_id}_multimap', 'bed')
        # Bedtools intersect with HERV GTF file (Multimap)
        subset_outsam_pathname = read_filter.get_path(
            'intermediate_files', f'{base_name}_{unique_id}_hervs_subset', 'sam')
        # Filter unique read appearances (step for KMER)
        subset_outbam_pathname = read_filter.get_path(
            'intermediate_files', f'{base_name}_{unique_id}_all_hervs.bwa_read', 'bam')

        commands = [
            f"samtools view -bS {outsam_pathname} -o {outbam_pathname}",
            f"samtools sort -@ {args.num_cores} {outbam_pathname} -o {sorted_outbam_pathname}",
            f"samtools index {sorted_outbam_pathname}",
            # Multimap uses this intermediate output below in the final dir
            f"bedtools intersect -abam {sorted_outbam_pathname} -b {converted_herv_gtf_to_bed} -wa -wb -bed > {outbed_pathname}",
            f"bedtools intersect -abam {sorted_outbam_pathname} -b {converted_herv_gtf_to_bed} > {subset_outbam_pathname}",
            # Kmer step uses this intermediate output in the final dir
            f"samtools view -h -o {subset_outsam_pathname} {subset_outbam_pathname}"
        ]
        if not args.b:
            # if bowtie file is not provided by user, add the bowtie 2 commands with unified mode parameters
            if paired:
                logging.info("Bowtie 2 - Unified Mode, paired reads")
                bt_cmd = f"""bowtie2 -p {args.num_cores} --very-sensitive --end-to-end -X 1500 \
                --no-mixed --no-discordant --no-dovetail --no-unal --score-min L,-0.1,-0.1 \
                -x {args.bowtie_index} -1 {read_filter.r1_path} -2 {read_filter.r2_path} -k 100 -S {outsam_pathname} \
                2> {read_filter.get_path('logs', base_name, f'bt2_paired_{unique_id}.err')}"""
            else:
                logging.info("Bowtie 2 - Unified Mode, single strand")
                bt_cmd = f"""bowtie2 -p {args.num_cores} -N 1 -L 10 --very-sensitive --end-to-end --no-unal \
                -x {args.bowtie_index} --score-min L,-0.1,-0.1 -U {read_filter.s1_path} -k 100 -S {outsam_pathname} \
                2> {read_filter.get_path('logs', base_name, f'bt2_single_{unique_id}.err')}"""
            # insert the bowtie 2 command to the front of the pipeline commands queue
            commands.insert(0, bt_cmd)

        logging.info("Starting processing pipeline...")
        logging.info(f"Running commands for file with hash: {unique_id}")
        for cmd in tqdm(commands, desc="Processing commands"):
            logging.info(f"\nExecuting: {cmd}")
            run_command(cmd)

        pathname_extracted_reads = read_filter.get_path(
            'intermediate_files', f'{base_name}_{unique_id}_extracted_reads', 'txt')
        # filter each read so that it appears once for KMER method
        logging.info("Extracting original reads for kmer analysis...")
        if paired:
            extract_out_original_reads_by_subset(
                read_filter.r1_path,
                subset_outsam_pathname,
                pathname_extracted_reads,
                read_filter.r2_path
            )
        else:
            extract_out_original_reads_by_subset(
                read_filter.s1_path,
                subset_outsam_pathname,
                pathname_extracted_reads
            )
        # filter each read so that it appears once for KMER method
        logging.info("KMER Method - 31 bp")
        if paired:
            # KMER STEP: Returns a dictionary of reads as keys and kmer assignment as values
            read_to_kmer_assignment_dict = parse_fastq_run_kmer_return_dict(input_sam_pathname=pathname_extracted_reads,
                                                                            kmer_herv_dict=kmer_herv_dict,
                                                                            path_dict=herv_path_dict,
                                                                            under_clade_dict=clades_under_dict,
                                                                            kmer_size=31,
                                                                            paired_end=True)
        else:
            read_to_kmer_assignment_dict = parse_fastq_run_kmer_return_dict(input_sam_pathname=pathname_extracted_reads,
                                                                            kmer_herv_dict=kmer_herv_dict,
                                                                            path_dict=herv_path_dict,
                                                                            under_clade_dict=clades_under_dict,
                                                                            kmer_size=31,
                                                                            paired_end=False)

        logging.info("Multimap Method")

        read_to_multimap_assignment_dict = single_multimapped_sam_to_dictionary(input_bed_pathname=outbed_pathname,
                                                                                path_dict=herv_path_dict)

        # resolve the differences between the kmer and the multimapping approaches.
        final_resolved_dict = resolve_reads_single_sample_final(kmer_dict=read_to_kmer_assignment_dict,
                                                                multi_dict=read_to_multimap_assignment_dict,
                                                                clade_dict=clades_under_dict)
        logging.info("Converting final dictionary into CSV")
        if paired:
            convert_final_dict_into_csv(original_fastq_pathname_for_normalization=args.r1,
                                        resolved_dict=final_resolved_dict,
                                        clade_dict=clades_under_dict,
                                        herv_path_dict=herv_path_dict,
                                        output_path=final_csv_out)
        else:
            convert_final_dict_into_csv(original_fastq_pathname_for_normalization=args.s1,
                                        resolved_dict=final_resolved_dict,
                                        clade_dict=clades_under_dict,
                                        herv_path_dict=herv_path_dict,
                                        output_path=final_csv_out)

        cleanup_intermediate_files(args.output_dir, args.keep_files)

    except Exception as e:
        logging.error(f"An error occurred: {str(e)}")
        raise


if __name__ == "__main__":
    main()
